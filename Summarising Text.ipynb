{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4290c377-5a80-4f5b-978f-0f92579878de",
   "metadata": {},
   "source": [
    "<center><img src=\"OCF logo.jpeg\" width=\"15%\" height=\"15%\" align=middle /><img src=\"nvidia-logo.png\" width=\"30%\" height=\"30%\" align=middle /><img src=\"runailogo.webp\" width=\"10%\" height=\"10%\" align=middle /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ad6f94-b308-480b-ac74-ffe1c609bdfb",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Hello and welcome to the OCF/NVIDIA/ Run:ai CIUK Cluster challenge 2022. \n",
    "\n",
    "The challenge is based on NVIDIA’s conversational AI using techniques such as automatic speech recognition (ASR) and Text to Speech (TTS) and natural Language processing (NLP). Run:ai will facilitate the environment to spin Jupiter Notebook and make GPU partitioning at a factional level. A 2U sturdy server with 3 NVIDIA 80GB at OCF labs is supporting the entire workloads. Each team are allocated a 4-hours’ time slot, 1 hour for prep and 3 hours for the challenge itself. The challenge is an open-ended challenge where you can bring your creative and technical knowledge through teamwork.  We will share you example scripts to familiarise with the actual software packages and to build an application or end to end proof of concept of your choice.  \n",
    "\n",
    "Please watch some NVIDIA videos about NVIDIA Riva and Triton inference server to learn more about the possibilities. \n",
    "\n",
    "https://www.nvidia.com/en-us/on-demand/session/gtcfall22-a41126/ \n",
    "\n",
    "https://www.nvidia.com/en-us/on-demand/session/gtcfall22-a41260/ \n",
    "\n",
    "Disclaimer - you might have to create an account if don’t already have one! \n",
    "\n",
    "\n",
    "## Connection instructions\n",
    "\n",
    "Each team members will be given a set of login details (you should have it by now) via email. The login details are used to log onto OCF hosted run:ai instance via https://ocf.run.ai. The run:ai Jupiter notebook is used to interact with NVIDIA Riva and Triton server. Each team are assigned to respective project name under Run:ai and each team member will have access to the assigned project, this way the team members can access other members notebooks etc.   \n",
    "\n",
    "\n",
    "## About Run.AI\n",
    "\n",
    "Run:ai provides a platform to spin up Jupiter notebook and helps to interact between Nvidia Riva and Kubernetes container. It also helps admins to manages user access and assign tasks within the project. \n",
    "\n",
    "Using the credentials please log onto https://ocf.run.ai and follow the video link for more instructions. (Password is CiukRunai) \n",
    "\n",
    "https://ocfoffice365-my.sharepoint.com/personal/vvijay_ocf_co_uk/_layouts/15/stream.aspx?id=%2Fpersonal%2Fvvijay%5Focf%5Fco%5Fuk%2FDocuments%2FMyDevice%2FDocuments%2FCIUK%20cluster%20challenge%2FRunai%20Intro%2Emov&wdLOR=c26042D37%2D9907%2D4360%2DBF22%2D24A15ABF81B8&ga=1 \n",
    "\n",
    "## Overview of the competition\n",
    "\n",
    "Since we have two competitions (Online and Onsite) we have designed the challenge such that they are split in two parts. Onsite (part 2) of the challenge will be the continuation of part 1 (Online). Part 1 will be focusing more on the planning and designing phase of the solution whereas the Part 2 will be the execution and implementation of the solution.  \n",
    "\n",
    "## Scoring Matrix \n",
    "\n",
    "### Part 1 (The Online challenge):  \n",
    "\n",
    "The challenge is open ended so think pout of the box, each team must come up with a creative solution design and present the most viable execution strategy using NVIDIA Riva and Triton inference server. The part one much include the following tasks and respective scoring matrices - \n",
    "\n",
    "1. Complete the example exercises - no points \n",
    "2. Implement the two new tasks (evidence for correct execution provided as screenshots) – 10 points \n",
    "3. Ask them to prepare a presentation on how they will bring the above technologies together to build something new – 40 points \n",
    "\n",
    "    a. Problem complexity – 5 points \n",
    "    \n",
    "    b. Problem novelty (backed by literature review) - 5 points \n",
    "    \n",
    "    c. Solution design (architectural flow mandatory) – 10 points \n",
    "    \n",
    "    d. Presentation (a recorded presentation video or pdf or PPT) – 10 points \n",
    "    \n",
    "    e. Additional points for project marketing (A team member to post your little video or results on LinkedIn tagging @OCF Limited @NVIDIA and @run:ai using #CIUK2022_SCC) - 10 points \n",
    "\n",
    "Maximum points - 50 \n",
    "\n",
    "All the results to be shared with vvijay@ocf.co.uk and adamg@nvidia.com soon after completion of allocated time slot. \n",
    " \n",
    " \n",
    "### Part 2: (The offline challenge) Friday 2 December (9am-12pm) \n",
    "\n",
    "The part 2 is the continuation of part 1, i.e., the solution designed in 3.c above will have to be implemented and presented in a 5 min demo + short presentation – 50 points \n",
    "\n",
    "1. Quality of the implementation (use of technology and complexity) – 25 points \n",
    "2. Recorded presentation of a demo (max 5 mins) – 25 points \n",
    "3. Brownly points for marketing (A team member to post your little video or results on LinkedIn tagging @OCF Limited @NVIDIA and @run:ai using #CIUK2022_SCC) - 10 points\n",
    "\n",
    "Maximum points – 50 \n",
    "\n",
    "All the results to be shared with vvijay@ocf.co.uk and adamg@nvidia.com soon after completion of allocated time slot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9599a091-1727-4a97-b47c-9ba492397a57",
   "metadata": {},
   "source": [
    "## The online challenge\n",
    "\n",
    "In the online challenge will be composed of the following activities:\n",
    "1. We will start by demonstrating:\n",
    "\n",
    "        a. How to execute individual services including Automatic Speech Recognition (ASR) and Text to Speech (TTS)\n",
    "        \n",
    "        b. How to adapt the above model to your use case (I.e., we will demonstrate word boosting helping the model deal with complex/rare words)\n",
    "        \n",
    "        c. How to execute a Large Language Model and in a few shot setting adapt it to carry out a new task. We will demonstrate how to teach it to translate and summarise text\n",
    "        \n",
    "2. Once you go through the prescribed examples, we will ask you to try your luck in (please provide screenshots or python files demonstrating a working solution):\n",
    "\n",
    "    a. Extending the summarisation example to summarise the entire chapter / long section of a book of your choice\n",
    "    \n",
    "    b. Extend the translation capability to a new language and translate of the above summary\n",
    "    \n",
    "3. Finally we will ask you to record a short video presentation capturing:\n",
    "\n",
    "    a. Your ideas for the conversational AI application you would like to implement during the offline part of the challenge\n",
    "    \n",
    "    b. Please make sure you include enough detail to help us shore your presentation using the criteria listed above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fcae28-c35e-4e60-b946-8e30b4f7c3ac",
   "metadata": {},
   "source": [
    "# The Environment\n",
    "\n",
    "The environment that you are using today is composed of several pieces of technology:\n",
    "1. You are now using a Jupiter Notebook which is hosted using the Run.AI environment. Every member of the competition has access to a subset of CPU cores in this system.\n",
    "2. Since we have a large group of users, and a limited number of GPUs the GPUs are shared between the challenge participants using two services:\n",
    "- Riva - which occupies one GPU and hosts several Natural Language Processing, Automatic Speech Recognition as well as Text to Speech models\n",
    "- Triton Inference Server - which occupies the second GPU and hosts a 5B parameter large GPT-3 using Faster Transformer execution backend.\n",
    "\n",
    "The environment setup is depicted below:<br/>\n",
    "<img src=\"SystemSetup.png\" width=\"60%\" height=\"60%\">\n",
    "\n",
    "In this class you will be interacting with those ecternall services to take advantage of the tools deployed in Riva and Triton. Before we dive in, let us make sure we have all the dependencies deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41705750-f875-4fd8-a88b-c5c10c21670c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install nvidia-riva-client\n",
    "!pip install numpy\n",
    "!pip install tritonclient[all]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825e66c-dcdc-436a-92dc-b37170f12fb6",
   "metadata": {},
   "source": [
    "# Riva: Foundation of Conversational AI\n",
    "\n",
    "Riva (https://docs.nvidia.com/deeplearning/riva/user-guide/docs/overview.html) is an end-to-end solution providing an implementation of the key Conversational AI pipelines including Automatic Speech Recognition, Text to Speech and a number of key Natural Language Processing Services. Those services can be used independently or be a part of a larger application. In the online part of the challenge, we will ask you to get familiar with the key conversational services and integrate them into a larger pipeline. We hope that this experience will help with the offline part of the challenge where you will be able to build a pipeline of your own.\n",
    "\n",
    "<br/><img src=\"riva-skills.png\" width=\"50%\" height=\"50%\">\n",
    "\n",
    "## Text to Speech\n",
    "In this part of the challenge lets get familiar with key services starting with the Text to Speech. You can find further examples of how to use speech services here: https://github.com/nvidia-riva/python-clients/blob/main/tutorials/TTS.ipynb\n",
    "\n",
    "We will start by connecting to our Riva Server and instantiating the speach synthesis client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1b1ed0-6eed-4f90-9cd5-2b800b834df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import riva.client\n",
    "auth = riva.client.Auth(uri='10.100.182.246:50051')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de8469-86c3-4e2d-a44b-34f1ec1e7025",
   "metadata": {},
   "outputs": [],
   "source": [
    "tts_service = riva.client.SpeechSynthesisService(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e689be06-9aaf-49a5-9fae-e9b4f4a9d691",
   "metadata": {},
   "source": [
    "With the client in place, we are ready to generate speech. Please generate the below text and listen to the quality of the generated audio file. Feel free to generate speech of your choice to test the limits of the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7548ae3-87a5-4fc2-bc83-87da14a16990",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_code = 'en-US'\n",
    "sample_rate_hz = 16000\n",
    "nchannels = 1\n",
    "sampwidth = 2\n",
    "text = (\n",
    "    \"The commonly known as the United States or America, \"\n",
    "    \"is a country primarily located in North America. It consists of 50 states, \"\n",
    "    \"a federal district, five major unincorporated territories, 326 Indian reservations, \"\n",
    "    \"and nine minor outlying islands.\" \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135d04d5-2c90-4bbd-a013-abf1b541b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = tts_service.synthesize(text, language_code=language_code, sample_rate_hz=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd790d41-7f12-4c6a-86b4-a13dca07b355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "offline_output_file = \"my_offline_synthesized_speech.wav\"\n",
    "with wave.open(offline_output_file, 'wb') as out_f:\n",
    "    out_f.setnchannels(nchannels)\n",
    "    out_f.setsampwidth(sampwidth)\n",
    "    out_f.setframerate(sample_rate_hz)\n",
    "    out_f.writeframesraw(resp.audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66713a71-7927-48a9-b3f2-3c498e1644aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(offline_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37b895f-2afc-47cf-ac99-8680bef2aad0",
   "metadata": {},
   "source": [
    "Riva provides extensive customisation capability for the generated text (using SSML language). The below example shows how to control Prosody (https://en.wikipedia.org/wiki/Prosody_(linguistics)) of the generated audio samples. For detailed documentation please refer to: https://docs.nvidia.com/deeplearning/riva/user-guide/docs/tutorials/tts-python-basics-and-customization-with-ssml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e3173-1d9f-494a-a46c-b068fe9e17bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "language_code = 'en-US'\n",
    "sample_rate_hz = 16000\n",
    "nchannels = 1\n",
    "sampwidth = 2\n",
    "text = (\n",
    "    \"\"\"<speak><prosody pitch='3.0'>Today is a sunny day</prosody>. <prosody rate='low' volume='+1dB'>But it might rain tomorrow.</prosody></speak>\"\"\"\n",
    ")\n",
    "resp = tts_service.synthesize(text, language_code=language_code, sample_rate_hz=sample_rate_hz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a271a890-c24b-41ba-b2ce-f7cd4893d229",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "offline_output_file = \"my_offline_synthesized_speech_prosody.wav\"\n",
    "with wave.open(offline_output_file, 'wb') as out_f:\n",
    "    out_f.setnchannels(nchannels)\n",
    "    out_f.setsampwidth(sampwidth)\n",
    "    out_f.setframerate(sample_rate_hz)\n",
    "    out_f.writeframesraw(resp.audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8bf381-abe0-4d57-85f4-b52367c0e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "IPython.display.Audio(offline_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dd8420-6162-4d2c-81cb-c49c0c308619",
   "metadata": {},
   "source": [
    "## Automatic Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e66a09-b023-435c-9d96-93843f6e9ae6",
   "metadata": {},
   "source": [
    "Let us now focus on Automatic Speech Recognition. For more examples, please refer to: https://github.com/nvidia-riva/python-clients/blob/main/tutorials/ASR.ipynb\n",
    "\n",
    "By now the flow should look familiar. Once again we will start by creating a client to connect to our server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b9cca6-dfdf-45a5-9d80-0a93a2e81565",
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_service = riva.client.ASRService(auth)\n",
    "\n",
    "offline_config = riva.client.RecognitionConfig(\n",
    "    encoding=riva.client.AudioEncoding.LINEAR_PCM,\n",
    "    max_alternatives=1,\n",
    "    enable_automatic_punctuation=True,\n",
    "    verbatim_transcripts=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382ce30-c1e8-43ac-bb4a-13d8ff6387bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wav_file = 'my_offline_synthesized_speech.wav'\n",
    "riva.client.add_audio_file_specs_to_config(offline_config, my_wav_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbe766b-78c7-4ab0-a1d3-796124370f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(my_wav_file, 'rb') as fh:\n",
    "    data = fh.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712aafb0-1049-4ed1-8ff8-a501b923ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = asr_service.offline_recognize(data, offline_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cbd1fa-b837-489c-b6a1-3e5bb3c68b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75134be8-34cb-4178-adcd-a82a33aec1cb",
   "metadata": {},
   "source": [
    "### Customising your ASR model - Word Boosting\n",
    "\n",
    "Speech recognition is a complex task. When trying to transcribe speech you will not only have to deal with challenging accents and environments but also an ever-evolving language and domain specific terms. Riva provides extensive customisability of your models (https://docs.nvidia.com/deeplearning/riva/user-guide/docs/asr/asr-customizing.html). In this example we will look at the simples of customisation options namely word boosting to fix transcription issues related to a domain specific vocabulary. Let’s try to genera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854ab8c3-777d-4e9b-9223-bf9a0a50ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"High-performance computing (HPC) uses supercomputers and computer clusters to solve advanced computation problems. \"\n",
    "    \"HPC integrates systems administration (including network and security knowledge) and parallel  \"\n",
    "    \"programming into a multidisciplinary field that combines digital electronics, computer architecture, \"\n",
    "    \"system software, programming languages, algorithms and computational techniques.\"\n",
    ")\n",
    "resp = tts_service.synthesize(text, language_code=language_code, sample_rate_hz=sample_rate_hz)\n",
    "offline_output_file = \"hpc_specific_synthesized_speech.wav\"\n",
    "with wave.open(offline_output_file, 'wb') as out_f:\n",
    "    out_f.setnchannels(nchannels)\n",
    "    out_f.setsampwidth(sampwidth)\n",
    "    out_f.setframerate(sample_rate_hz)\n",
    "    out_f.writeframesraw(resp.audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48d86cc-94ac-4ed5-8b46-a5642845ad69",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wav_file = 'hpc_specific_synthesized_speech.wav'\n",
    "riva.client.add_audio_file_specs_to_config(offline_config, my_wav_file)\n",
    "with open(my_wav_file, 'rb') as fh:\n",
    "    data = fh.read()\n",
    "response = asr_service.offline_recognize(data, offline_config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6d685-0895-40d7-922c-f7d2190ed95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(offline_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60f4f1a-ad9f-4048-b17c-893da8e2c924",
   "metadata": {},
   "source": [
    "Please note that the term HPC was not recognised correctly as it is not part of the default model vocabulary. Let’s address that by Boosting the term (https://github.com/nvidia-riva/tutorials/blob/stable/asr-python-advanced-wordboosting.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052e16e-7554-40a8-856d-a8157b553ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "boosted_lm_words = ['HPC']\n",
    "boosted_lm_score = 20.0\n",
    "riva.client.add_word_boosting_to_config(offline_config, boosted_lm_words, boosted_lm_score)\n",
    "riva.client.add_audio_file_specs_to_config(offline_config, my_wav_file)\n",
    "with open(my_wav_file, 'rb') as fh:\n",
    "    data = fh.read()\n",
    "response = asr_service.offline_recognize(data, offline_config)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1f869b-cfdd-4ca6-801f-b0e426045d1e",
   "metadata": {},
   "source": [
    "## NLP Services\n",
    "\n",
    "Last 3 years have seen unprecedented progress in Natural Language Processing. Very large language model gained few-shot learning capability meaning that they can learn to carry out tasks with just few learning examples. In this part of the class, we will look at both medium sized models (i.e. BERT) as well as a large GPT model composed of 6 Billion Parameters. Let us start with prebuild Riva based NLP services."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a8f909-f968-472b-b3ad-8d6e497e89a5",
   "metadata": {},
   "source": [
    "## Intent prediction and slot filling\n",
    "\n",
    "Out of the box Riva comes with example intent prediction models covering a very narrow set of domains. The below examples demonstrate the structure of the query for joint intent prediction and slot filling task. For more examples on how to use NLP service refer to: https://github.com/nvidia-riva/python-clients/blob/main/tutorials/NLP.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b5ed5c-05e9-479e-9558-d2fdee0ced75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from riva.client.proto.riva_nlp_pb2 import (\n",
    "    AnalyzeIntentResponse,\n",
    "    NaturalQueryResponse,\n",
    "    TextClassResponse,\n",
    "    TextTransformResponse,\n",
    "    TokenClassResponse,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c17d35-f87d-4763-8606-53d97197d375",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_service = riva.client.NLPService(auth)\n",
    "response = nlp_service.analyze_intent(\n",
    "    input_string=\"How is the weather today in New England\",\n",
    "    options=riva.client.AnalyzeIntentOptions(lang='en-US'),\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0362a480-9301-40e3-b2e4-e0909e45ecfa",
   "metadata": {},
   "source": [
    "## Punctuation and capitalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e96a7f-1983-4e02-a3ac-d84bc96f7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation_queries = [\n",
    "    \"by the early 20th century the gar complained more and more about the younger generation\",\n",
    "#     \"boa Vista is the capital of the brazilian state of roraima situated on the western bank of \"\n",
    "#     \"the branco river the city lies 220 km from brazil's border with venezuela.\",\n",
    "]\n",
    "response: TextTransformResponse = nlp_service.punctuate_text(punctuation_queries)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228f9d8f-21e9-4788-a6da-dfbc4a49e550",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055a97e7-000a-42a1-a5d5-8e2c8cfa7d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_query = \"How many gigatons of carbon dioxide was released in 2005?\"\n",
    "qa_context = (\n",
    "    \"In 2010 the Amazon rainforest experienced another severe drought, in some ways more extreme than the \"\n",
    "    \"2005 drought. The affected region was approximate 1,160,000 square miles (3,000,000 km2) of \"\n",
    "    \"rainforest, compared to 734,000 square miles (1,900,000 km2) in 2005. The 2010 drought had three \"\n",
    "    \"epicenters where vegetation died off, whereas in 2005 the drought was focused on the southwestern \"\n",
    "    \"part. The findings were published in the journal Science. In a typical year the Amazon absorbs 1.5 \"\n",
    "    \"gigatons of carbon dioxide; during 2005 instead 5 gigatons were released and in 2010 8 gigatons were \"\n",
    "    \"released.\"\n",
    ")\n",
    "response: NaturalQueryResponse = nlp_service.natural_query(qa_query, qa_context)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e2d755-bc73-4907-b067-14cee1d01ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = response.results[0].answer\n",
    "score = response.results[0].score\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6b95f-97b0-4a51-9071-40125bd2c93b",
   "metadata": {},
   "source": [
    "## NLP with Triton\n",
    "\n",
    "This example demonstrates zero and few-shot learning capability of 6B GPT model.\n",
    "\n",
    "### First program - zero shot learning\n",
    "\n",
    "Let’s establish the basic functionality required to query our model. The model is hosted on Triton Inference Server (https://github.com/triton-inference-server/server) which is an open source inferencing solution very broadly used to host end-to-end machine learning pipelines on GPU but also CPU based systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1f291e-a676-4e54-9549-b481c8c7d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tritonclient.http as httpclient\n",
    "\n",
    "from tritonclient.utils import np_to_triton_dtype\n",
    "\n",
    "URL = \"10.98.96.143:8000\"\n",
    "MODEl_GPTJ_FASTERTRANSFORMER = \"ensemble\" \n",
    "\n",
    "OUTPUT_LEN = 128\n",
    "BATCH_SIZE = 1\n",
    "BEAM_WIDTH = 1\n",
    "TOP_K = 1.0\n",
    "TOP_P = 0.0\n",
    "\n",
    "start_id = 220\n",
    "end_id = 50256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90709420-036c-4ffd-a3e7-67f837271cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = httpclient.InferenceServerClient(URL,\n",
    "                                           concurrency=1,\n",
    "                                           verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5a0f0f-e005-4845-a7b8-b1ce477aac2c",
   "metadata": {},
   "source": [
    "Let us make sure we can correctly connect to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb495c-db6f-4f1e-b967-cd5735cac444",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.get_server_metadata()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8680be-c386-4f37-b7cd-42154b9fdef5",
   "metadata": {},
   "source": [
    "When generating text using GPT we do have several options that we can set to controll the process. The below code will set some default values for those parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662a258-6da7-45ac-b337-a248944461dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference hyperparameters\n",
    "def prepare_tensor(name, input):\n",
    "    tensor = httpclient.InferInput(\n",
    "        name, input.shape, np_to_triton_dtype(input.dtype))\n",
    "    tensor.set_data_from_numpy(input)\n",
    "    return tensor\n",
    "\n",
    "# explanation\n",
    "def prepare_inputs(input0):\n",
    "    bad_words_list = np.array([[\"\"]], dtype=object)\n",
    "    stop_words_list = np.array([[\"\"]], dtype=object)\n",
    "    input0_data = np.array(input0).astype(object)\n",
    "    output0_len = np.ones_like(input0).astype(np.uint32) * OUTPUT_LEN\n",
    "    runtime_top_k = (TOP_K * np.ones([input0_data.shape[0], 1])).astype(np.uint32)\n",
    "    runtime_top_p = TOP_P * np.ones([input0_data.shape[0], 1]).astype(np.float32)\n",
    "    beam_search_diversity_rate = 0.0 * np.ones([input0_data.shape[0], 1]).astype(np.float32)\n",
    "    temperature = 1.0 * np.ones([input0_data.shape[0], 1]).astype(np.float32)\n",
    "    len_penalty = 1.0 * np.ones([input0_data.shape[0], 1]).astype(np.float32)\n",
    "    repetition_penalty = 1.0 * np.ones([input0_data.shape[0], 1]).astype(np.float32)\n",
    "    random_seed = 0 * np.ones([input0_data.shape[0], 1]).astype(np.int32)\n",
    "    is_return_log_probs = True * np.ones([input0_data.shape[0], 1]).astype(bool)\n",
    "    beam_width = (BEAM_WIDTH * np.ones([input0_data.shape[0], 1])).astype(np.uint32)\n",
    "    start_ids = start_id * np.ones([input0_data.shape[0], 1]).astype(np.uint32)\n",
    "    end_ids = end_id * np.ones([input0_data.shape[0], 1]).astype(np.uint32)\n",
    "\n",
    "    inputs = [\n",
    "        prepare_tensor(\"INPUT_0\", input0_data),\n",
    "        prepare_tensor(\"INPUT_1\", output0_len),\n",
    "        prepare_tensor(\"INPUT_2\", bad_words_list),\n",
    "        prepare_tensor(\"INPUT_3\", stop_words_list),\n",
    "        prepare_tensor(\"runtime_top_k\", runtime_top_k),\n",
    "        prepare_tensor(\"runtime_top_p\", runtime_top_p),\n",
    "        prepare_tensor(\"beam_search_diversity_rate\", beam_search_diversity_rate),\n",
    "        prepare_tensor(\"temperature\", temperature),\n",
    "        prepare_tensor(\"len_penalty\", len_penalty),\n",
    "        prepare_tensor(\"repetition_penalty\", repetition_penalty),\n",
    "        prepare_tensor(\"random_seed\", random_seed),\n",
    "        prepare_tensor(\"is_return_log_probs\", is_return_log_probs),\n",
    "        prepare_tensor(\"beam_width\", beam_width),\n",
    "        prepare_tensor(\"start_id\", start_ids),\n",
    "        prepare_tensor(\"end_id\", end_ids),\n",
    "    ]\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b906b18-2467-479b-9433-9a9cf75dea40",
   "metadata": {},
   "source": [
    "With that let us query our model. In this case we will ask it to generate text describing the process of writing conversational AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec68bc5-214e-4f17-987d-caab9080bd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user = \"How do I write an awesome Conversational AI application?\"\n",
    "input0 = [[input_user],]\n",
    "inputs = prepare_inputs(input0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c458288-06e5-4f9f-8640-a7afbc2de055",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = client.infer(MODEl_GPTJ_FASTERTRANSFORMER, inputs)\n",
    "output0 = result.as_numpy(\"OUTPUT_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d863c317-4ca0-4dc8-b9a2-7394d6328959",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output0[0].decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc832f8-71d7-43f4-89a6-8f08f7fa65dd",
   "metadata": {},
   "source": [
    "### Few shot learning\n",
    "\n",
    "The above was fascinating but not very useful. Let us look at the process of few-shot learning. We will teach out model how to translate with just three training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a823d5-c4c8-4c87-a1b7-98bf36cb86de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user = (\n",
    "    \"Polish: Gdzie jest najbliższa stacja kolejowa? \\n\"\n",
    "    \"English: Where is the nearest train station? \\n\\n\"\n",
    "    \"Polish: Ile kosztuje bilet kolejowy do Warszawy? \\n\"\n",
    "    \"English: How much is a train ticket to Warsaw? \\n\\n\"\n",
    "    \"Polish: Niestety nie mówię po Francusku. \\n\"\n",
    "    \"English: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51519b-f9cb-482f-8fdd-d392fca1b4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = [[input_user],]\n",
    "inputs = prepare_inputs(input0)\n",
    "result = client.infer(MODEl_GPTJ_FASTERTRANSFORMER, inputs)\n",
    "output0 = result.as_numpy(\"OUTPUT_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6b2a2a-5c65-4bb4-90e5-c5130a7f476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output0[0].decode('UTF-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3823b770-9dc2-4edf-9157-0bc755f347f6",
   "metadata": {},
   "source": [
    "### Text Summarisation\n",
    "\n",
    "Similarly, we can teach out model to summarise. The below is a zero shot learning example (we encourage our model to summarise by using the keyword tl;dr. Hopefully you now also understand how to reinforce this capability with a one / few shot setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169bbaf8-86ba-4f8c-84df-0cadcc7f7e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_user = (\"\"\"The want of an interesting work on Greek and Roman mythology, suitable for\n",
    "the requirements of both boys and girls, has long been recognized by the\n",
    "principals of our advanced schools. The study of the classics themselves,\n",
    "even where the attainments of the pupil have rendered this feasible, has\n",
    "not been found altogether successful in giving to the student a clear and\n",
    "succinct idea of the religious beliefs of the ancients, and it has been\n",
    "suggested that a work which would so deal with the subject as to render it\n",
    "at once interesting and instructive would be hailed as a valuable\n",
    "introduction to the study of classic authors, and would be found to assist\n",
    "materially the labours of both master and pupil.\n",
    "\n",
    "In endeavouring to supply this want I have sought to place before the\n",
    "reader a lifelike picture of the deities of classical times as they were\n",
    "conceived and worshipped by the ancients themselves, and thereby to awaken\n",
    "in the minds of young students a desire to become more intimately\n",
    "acquainted with the noble productions of classical antiquity.\n",
    "\n",
    "It has been my aim to render the Legends, which form the second portion of\n",
    "the work, a picture, as it were, of old Greek life; its customs, its\n",
    "superstitions, and its princely hospitalities, for which reason they are\n",
    "given at somewhat greater length than is usual in works of the kind.\n",
    "\n",
    "In a chapter devoted to the purpose some interesting particulars have been\n",
    "collected respecting the public worship of the ancient Greeks and Romans\n",
    "(more especially of the former), to which is subjoined an account of their\n",
    "principal festivals.\n",
    "\n",
    "I may add that no pains have been spared in order that, without passing\n",
    "over details the omission of which would have {ii} marred the completeness\n",
    "of the work, not a single passage should be found which could possibly\n",
    "offend the most scrupulous delicacy; and also that I have purposely treated\n",
    "the subject with that reverence which I consider due to every religious\n",
    "system, however erroneous.\n",
    "\n",
    "It is hardly necessary to dwell upon the importance of the study of\n",
    "Mythology: our poems, our novels, and even our daily journals teem with\n",
    "classical allusions; nor can a visit to our art galleries and museums be\n",
    "fully enjoyed without something more than a mere superficial knowledge of a\n",
    "subject which has in all ages inspired painters, sculptors, and poets. It\n",
    "therefore only remains for me to express a hope that my little work may\n",
    "prove useful, not only to teachers and scholars, but also to a large class\n",
    "of general readers, who, in whiling away a leisure hour, may derive some\n",
    "pleasure and profit from its perusal.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc4b026-1f9d-4daf-a4c5-46651570d99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "TOP_K = 0.3\n",
    "TOP_P = 0.8\n",
    "\n",
    "def letters(strng):\n",
    "    tot = 0\n",
    "    for i in strng:\n",
    "        if i.isalpha() == True:\n",
    "            tot += 1\n",
    "    return tot\n",
    "\n",
    "for p in input_user.split(\"\\n\\n\"):\n",
    "    input0 = [[p],]\n",
    "    inputs = prepare_inputs(input0)\n",
    "    result = client.infer(MODEl_GPTJ_FASTERTRANSFORMER, inputs)\n",
    "    output0 = result.as_numpy(\"OUTPUT_0\")\n",
    "    strOut = output0[0].decode('UTF-8').split(\"\\n\\n\")\n",
    "    strOut[0] = \"\"\n",
    "    final = \"\"\n",
    "    for s in strOut:\n",
    "        if letters(s) > letters(final):\n",
    "            final = s\n",
    "    print(final, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc27cba-61f9-462d-87ee-d0d9e60c350f",
   "metadata": {},
   "source": [
    "Congratulations! You have written a simple summarisation algorithm. How would you now improve it? Some ideas include:\n",
    "    - Obviously more examples.\n",
    "    - Better decoding strategy (please read the following https://huggingface.co/blog/how-to-generate and try to experiment with top-p decoding which should make the output less repetitive)\n",
    "    - Approaches for more efficient in context learning (https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/prompt_learning.html)  or Parameter Efficient Fine Tunning (https://arxiv.org/abs/2205.05638)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b0b87-7336-4fda-985b-36c5020f0032",
   "metadata": {},
   "source": [
    "# Online challenge\n",
    "\n",
    "Now its your turn to write some code. To get the challenge points please go through the below tasks one by one:\n",
    "1. Implement translation to any other language\n",
    "2. Extend the summarisation example to longer text\n",
    "3. Integrate Automatic Speech Recognition, summarisation and translation into a single coherent pipeline. See below diagram for an example:\n",
    "<img src=\"va-framework.png\" width=\"60%\" height=\"60%\">\n",
    "4. Think about how you will use those technologies together in an online challenge\n",
    "5. Record a video presentation capturing your idea and technologies you will use for implementation\n",
    "6. If possible, share your proposal in social media\n",
    "\n",
    "Please review once again the scoring matrix. Good luck with the challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b44304-a835-4de5-b650-dbed26600308",
   "metadata": {},
   "source": [
    "# Offline Challenge\n",
    "\n",
    " We will meet again during offline challenge at CIUK 2022 Cluster challenge. OCF NVIDIA and Run:ai challenge is scheduled between Friday 2 December 9am-12pm. \n",
    "On this day you will execute the part 2 of the challenge that is the continuation part 1 from online challenge. Each team members will have respective login for Run:ai and a chance to build your suggested Riva solution similar to online challenge. At the end of the challenge the teams should have implemented a solution design proposed in part 1 and demo the working model of the solution via a recorded session (max 5 mins). At the end the solutions are scored by our experts, please refer to the scoring matrix for more details.\n",
    "By then you should have achieved a strong knowledge and understanding of conversational AI (ASR, TTS and NLP) using NVDIA Riva with the help of Run:ai and OCF.\n",
    "\n",
    "Later in the day at 2 pm we will have final winner of the CIUK 2022 cluster challenge will be announced. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be8363a-750a-48a3-b1e5-c8d6616ff031",
   "metadata": {},
   "source": [
    "<center><img src=\"OCF logo.jpeg\" width=\"15%\" height=\"15%\" align=middle /><img src=\"nvidia-logo.png\" width=\"30%\" height=\"30%\" align=middle /><img src=\"runailogo.webp\" width=\"10%\" height=\"10%\" align=middle /></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
